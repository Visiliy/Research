import os
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from tokenizers import ByteLevelBPETokenizer
from tqdm import tqdm
import time
import gc
import signal
import sys

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–ª–∞–≥ –¥–ª—è graceful shutdown
interrupted = False

def signal_handler(sig, frame):
    global interrupted
    print("\n[INFO] Interrupt received, saving progress and exiting...")
    interrupted = True

signal.signal(signal.SIGINT, signal_handler)

# –í–∫–ª—é—á–∞–µ–º expandable_segments (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# ========================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ========================

def read_file_batch(file_paths):
    """–ß—Ç–µ–Ω–∏–µ –±–∞—Ç—á–∞ —Ñ–∞–π–ª–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø—É—Å—Ç—ã—Ö"""
    results = []
    for fp in file_paths:
        try:
            with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:
                content = fh.read().strip()
                if content:
                    results.append((str(fp), content))
        except Exception as e:
            continue
    return results

def tokenize_batch(tokenizer, batch_texts):
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
    return [tokenizer.encode(text).ids for text in batch_texts]

def read_texts_from_folder(folder):
    """–ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤"""
    p = Path(folder)
    if not p.exists():
        return []
    return list(p.rglob('*.txt'))

def process_files_parallel(files, batch_size=5000, max_workers=None):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ"""
    if max_workers is None:
        max_workers = min(os.cpu_count() or 4, 8)
    
    file_paths = [str(f) for f in files]
    all_results = []
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for i in range(0, len(file_paths), batch_size):
            batch = file_paths[i:i + batch_size]
            futures.append(executor.submit(read_file_batch, batch))
        
        for i, future in enumerate(as_completed(futures)):
            try:
                batch_results = future.result()
                if batch_results:
                    all_results.extend(batch_results)
            except Exception as e:
                continue
            if i % 10 == 0:
                print(f"Processed {len(all_results)} files")
            if interrupted:
                break
    
    return all_results

def train_tokenizer_sequential(texts, vocab_size=50000):
    """–û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ —É–∂–µ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö"""
    temp_file = "temp_tokenizer_texts.txt"
    try:
        with open(temp_file, 'w', encoding='utf-8') as f:
            for text in texts:
                f.write(text + '\n')
        
        tokenizer = ByteLevelBPETokenizer()
        tokenizer.train(
            files=[temp_file], 
            vocab_size=vocab_size, 
            min_frequency=2, 
            special_tokens=['<s>', '</s>', '<pad>', '<unk>', '<mask>']
        )
        return tokenizer
    finally:
        if os.path.exists(temp_file):
            os.remove(temp_file)

def tokenize_sequential(tokenizer, texts, batch_size=1000):
    """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"""
    all_indices = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Tokenizing"):
        if interrupted:
            break
        batch_texts = texts[i:i + batch_size]
        batch_indices = [tokenizer.encode(text).ids for text in batch_texts]
        all_indices.extend(batch_indices)
    return all_indices

class EfficientTextEmbedding(nn.Module):
    def __init__(self, vocab_size, emb_dim):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
    
    def forward(self, x):
        return self.emb(x)

def get_positional_encoding(seq_len, dim, device):
    """–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)"""
    pos = torch.arange(seq_len, dtype=torch.float32, device=device).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, dim, 2, dtype=torch.float32, device=device) * 
                        (-np.log(10000.0) / dim))
    pe = torch.zeros(seq_len, dim, device=device)
    pe[:, 0::2] = torch.sin(pos * div_term)
    pe[:, 1::2] = torch.cos(pos * div_term)
    return pe

# ========================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ë–†–ê–ë–û–¢–ö–ò
# ========================

def process_gpu_batches(model, indices, file_paths, batch_size, device, emb_dim, save_dir="embeddings"):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ GPU —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –Ω–∞ –¥–∏—Å–∫ ‚Äî –ù–ò–ß–ï–ì–û –ù–ï –î–ï–†–ñ–ò–ú –í –ü–ê–ú–Ø–¢–ò"""
    os.makedirs(save_dir, exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    processed_indices = set()
    embedding_files = sorted([f for f in os.listdir(save_dir) if f.startswith("batch_") and f.endswith(".pt")])
    for fname in embedding_files:
        try:
            start_idx = int(fname.split("_")[1])
            end_idx = int(fname.split("_")[2].split(".")[0])
            for i in range(start_idx, end_idx):
                processed_indices.add(i)
        except:
            continue

    if len(processed_indices) > 0:
        print(f"Resuming from {len(processed_indices)} processed files")

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    max_len = 512
    pos_emb = get_positional_encoding(max_len, emb_dim, device)
    
    model.eval()
    torch.cuda.empty_cache()
    gc.collect()

    total_processed = len(processed_indices)
    start_time = time.time()

    with torch.no_grad():
        for i in tqdm(range(0, len(indices), batch_size), desc="GPU Processing", initial=total_processed//batch_size):
            if interrupted:
                break

            start_idx = i
            end_idx = min(i + batch_size, len(indices))
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
            if all(idx in processed_indices for idx in range(start_idx, end_idx)):
                continue

            batch_indices = indices[start_idx:end_idx]
            batch_files = file_paths[start_idx:end_idx]
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–∞ —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º
            batch_tensors = []
            for seq in batch_indices:
                if len(seq) > max_len:
                    seq = seq[:max_len]
                else:
                    seq = seq + [0] * (max_len - len(seq))
                batch_tensors.append(seq)
            
            x = torch.tensor(batch_tensors, dtype=torch.long, device=device)
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            text_emb = model(x)
            combined_emb = text_emb + pos_emb.unsqueeze(0)  # Broadcasting
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫ —Å—Ä–∞–∑—É ‚Äî –ù–ï –î–ï–†–ñ–ò–ú –í –ü–ê–ú–Ø–¢–ò!
            batch_output_path = os.path.join(save_dir, f"batch_{start_idx}_{end_idx}.pt")
            torch.save({
                'embeddings': combined_emb.cpu(),  # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ CPU –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                'files': batch_files,
                'start_idx': start_idx,
                'end_idx': end_idx
            }, batch_output_path)
            
            # –û—á–∏—Å—Ç–∫–∞
            del x, text_emb, combined_emb
            torch.cuda.empty_cache()
            gc.collect()
            
            # –õ–æ–≥ –ø–∞–º—è—Ç–∏ GPU
            if device.type == 'cuda':
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"  GPU Memory: Allocated {allocated:.2f} GB, Reserved {reserved:.2f} GB")

            total_processed = end_idx
            elapsed = time.time() - start_time
            speed = total_processed / elapsed if elapsed > 0 else 0
            print(f"  Saved batch {start_idx}-{end_idx} ({total_processed}/{len(indices)} files, {speed:.1f} files/s)")

    # –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è ‚Äî —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å–Ω—ã–π —Ñ–∞–π–ª
    all_files = []
    all_embeddings = []  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –∫–æ–Ω—Ü–µ ‚Äî –∏–Ω–∞—á–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ!

    # –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å –ø–∞–º—è—Ç—å—é!)
    embedding_files = sorted([f for f in os.listdir(save_dir) if f.startswith("batch_") and f.endswith(".pt")],
                             key=lambda x: int(x.split("_")[1]))
    
    for fname in embedding_files:
        data = torch.load(os.path.join(save_dir, fname), map_location='cpu')
        all_files.extend(data['files'])
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å ‚Äî —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ, –Ω–æ —Ä–∏—Å–∫—É–µ—Ç–µ OOM:
        # all_embeddings.append(data['embeddings'])
    
    # –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª ‚Äî –¥–µ–ª–∞–π—Ç–µ —ç—Ç–æ –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
    # if all_embeddings:
    #     final_embeddings = torch.cat(all_embeddings, dim=0)
    #     torch.save({
    #         'embeddings': final_embeddings,
    #         'files': all_files
    #     }, os.path.join(save_dir, "all_embeddings.pt"))
    #     print(f"Final embeddings saved: {final_embeddings.shape}")

    print(f"‚úÖ All batches processed and saved to '{save_dir}'")
    return None, all_files  # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º ‚Äî –æ–Ω–∏ –Ω–∞ –¥–∏—Å–∫–µ

# ========================
# MAIN
# ========================

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    folder = 'amps'
    files = read_texts_from_folder(folder)
    
    if not files:
        print("No files found")
        return
    
    print(f"Found {len(files):,} files")
    
    # –≠—Ç–∞–ø 1: –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    start_time = time.time()
    file_data = process_files_parallel(files, batch_size=5000)
    
    if not file_data:
        print("No valid text content found")
        return
    
    file_paths = [data[0] for data in file_data]
    texts = [data[1] for data in file_data]
    
    read_time = time.time() - start_time
    print(f"Reading completed: {len(texts):,} files in {read_time:.1f}s ({len(texts)/read_time:.1f} files/s)")
    
    # –≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("Training tokenizer...")
    tokenizer = train_tokenizer_sequential(texts)
    vocab_size = tokenizer.get_vocab_size()
    print(f"Tokenizer trained with vocab size: {vocab_size}")
    tokenizer.save("tokenizer.json")
    
    # –≠—Ç–∞–ø 3: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    print("Tokenizing texts...")
    tokenize_time = time.time()
    indices = tokenize_sequential(tokenizer, texts, batch_size=1000)
    tokenize_time = time.time() - tokenize_time
    print(f"Tokenization completed in {tokenize_time:.1f}s")
    
    if interrupted:
        print("üõë Stopped by user during tokenization")
        return

    # –≠—Ç–∞–ø 4: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ GPU
    emb_dim = 512
    model = EfficientTextEmbedding(vocab_size, emb_dim).to(device)
    
    # ‚ö†Ô∏è –ö–ª—é—á–µ–≤–æ–µ: —É–º–µ–Ω—å—à–∞–µ–º –±–∞—Ç—á –¥–æ 32 –∏–ª–∏ 64 –¥–ª—è 8GB GPU
    gpu_batch_size = 32  # –ë—ã–ª–æ 256 ‚Äî —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ!
    
    print(f"Starting GPU processing with batch_size={gpu_batch_size}")
    _, processed_files = process_gpu_batches(
        model, indices, file_paths, gpu_batch_size, device, emb_dim, save_dir="amps_embeddings"
    )
    
    total_time = time.time() - start_time
    print(f"‚úÖ Processing completed: {len(processed_files):,} files in {total_time:.1f}s")
    print(f"üìä Average speed: {len(processed_files)/total_time:.1f} files/s")

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\nüõë Process interrupted by user.")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        raise